# Data Import Pipeline

A comprehensive plan for intelligent data import, cleaning, extraction, and combination in ipyfiledrop.

## Overview

Real-world data files often contain messy, sparse layouts with metadata headers, footer rows, and core data tables buried within. This pipeline provides tools to:

1. **Extract** - Identify and extract core data tables from messy files
2. **Clean** - Normalize, standardize, and validate extracted data
3. **Combine** - Merge multiple similar DataFrames into unified datasets

## Architecture

```
Raw File(s)
    │
    ▼
┌─────────────────────────────────────┐
│  1. EXTRACT CORE DATA               │
│     - Density analysis              │
│     - Header detection              │
│     - Metadata extraction           │
│     - Footer separation             │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│  2. CLEAN CORE DATA                 │
│     - Normalize column names        │
│     - Strip whitespace              │
│     - Standardize NA values         │
│     - Infer/convert types           │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│  3. VALIDATE (Optional)             │
│     - Schema check                  │
│     - Confidence threshold          │
│     - Warn on issues                │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│  4. COMBINE (if multiple files)     │
│     - Concat core data              │
│     - Merge metadata as columns     │
│     - Track source files            │
└─────────────────────────────────────┘
    │
    ▼
Clean Combined DataFrame + Metadata
```

---

## Phase 1: Core Data Extraction

### Problem Statement

Raw files often look like this (from `examples/data/sparse_messy/sample_log.csv`):

```
Row,,,,Notes,,,
1,Report Date,2024-01-15,,,,,
2,Generated By,Lab System v2.1,,,,,
3,,,,,,,
4,,,,,,,
5,Lab,Sample ID,Test Type,Result,Units,Technician,Comments
6,,SAMP-001,pH,7.0,pH,A. Jones,slight deviation
7,,SAMP-001,Turbidity,2.9,NTU,,high reading
...
28,,,,End of Report,,,
29,,,,Approved: Dr. Wilson,,,
```

**Challenges:**
- Header row is at row 5, not row 0
- Data starts at row 6
- First column contains row numbers (junk)
- Empty rows scattered throughout
- Metadata in rows 1-2
- Footer in rows 28-29

### Detection Strategies

#### 1. Density Analysis

Calculate "density" (fraction of non-empty cells) per row and column:

```python
def calculate_density(series):
    """Fraction of non-null, non-empty cells."""
    valid = sum(1 for v in series if pd.notna(v) and str(v).strip())
    return valid / len(series)
```

Core data regions have consistently high density (>0.5), while metadata/footer regions are sparse (<0.3).

#### 2. Header Detection

Headers are rows where:
- Most cells are non-empty strings
- Values are unique (not repeated data patterns)
- Followed by rows with different types (numeric data)

```python
def is_likely_header(row, following_rows):
    all_strings = all(isinstance(v, str) for v in row if pd.notna(v))
    types_change = infer_types(row) != infer_types(following_rows[0])
    return all_strings and types_change
```

#### 3. Rectangular Block Detection

Find the largest contiguous rectangle with high density:

```
1. Create binary mask: 1 if cell non-empty, 0 otherwise
2. Find contiguous rows with density > threshold
3. Within those rows, find columns with density > threshold
4. Result is the "core" rectangle
```

### Algorithm: `extract_core_data()`

```python
def extract_core_data(df: pd.DataFrame, 
                      density_threshold: float = 0.5) -> ExtractedData:
    """
    Extract core data table from messy DataFrame.
    
    Returns:
        ExtractedData with:
        - core: The main data table (DataFrame)
        - metadata: Key-value pairs from header region (dict)
        - header_row: Original row index of detected header
        - data_range: (start_row, end_row) of core data
        - footer: Content from footer region (list)
        - confidence: 0-1 extraction quality score
    """
```

**Steps:**
1. Calculate row densities
2. Find contiguous dense region (core candidate)
3. Detect header row (first row of dense region, all strings)
4. Extract metadata from rows above core
5. Extract footer from rows below core
6. Slice and return core DataFrame with proper headers

### Data Structures

```python
@dataclass
class ExtractedData:
    core: pd.DataFrame              # Main data table
    metadata: Dict[str, Any]        # Key-value metadata
    header_row: Optional[int]       # Original header row index
    data_range: Tuple[int, int]     # (start, end) row indices
    footer: List[str]               # Footer content
    confidence: float               # 0-1 quality score
    warnings: List[str]             # Issues detected
```

### Metadata Extraction Patterns

| Pattern | Example | Detection |
|---------|---------|-----------|
| Key: Value | `Report Date: 2024-01-15` | Regex `^(.+?):\s*(.+)$` |
| Key = Value | `Version = 2.1` | Regex `^(.+?)\s*=\s*(.+)$` |
| Label in A, Value in B | Row with 2 non-empty cells | Positional |
| Key-value pairs | `Generated By, Lab System` | Cell followed by value |

---

## Phase 2: Data Cleaning

### Built-in Cleaners

| Cleaner | Description | Default |
|---------|-------------|---------|
| `normalize_columns` | Lowercase, strip, replace spaces/special chars with `_` | Yes |
| `strip_whitespace` | Strip leading/trailing whitespace from strings | Yes |

### Cleaner Options

Use factory functions for customized cleaners:

**`make_normalize_columns()`**
- `preserve_case=True`: Keep original case (default: lowercase)
- `preserve_dashes=True`: Keep dashes `-` (default: replace with `_`)
- `preserve_dots=True`: Keep dots `.` (default: replace with `_`)

```python
from ipyfiledrop import make_normalize_columns

# Custom column normalization
custom_normalize = make_normalize_columns(
    preserve_case=True, 
    preserve_dashes=True
)
fd = FileDrop("Data", cleaners=[custom_normalize])
```

**`make_strip_whitespace()`**
- `normalize_inner=True`: Collapse multiple inner spaces to single space

```python
from ipyfiledrop import make_strip_whitespace

# Strip whitespace and normalize inner spaces
custom_strip = make_strip_whitespace(normalize_inner=True)
fd = FileDrop("Data", cleaners=[custom_strip])
```
| `drop_empty_rows` | Remove rows where all values are NaN/empty | Yes |
| `drop_empty_cols` | Remove columns where all values are NaN/empty | Yes |
| `standardize_na` | Convert "N/A", "-", "null", "" to NaN | Yes |
| `deduplicate` | Remove duplicate rows | No |
| `infer_types` | Convert object columns to numeric/datetime | No |

### Cleaning Presets

```python
CLEANING_PRESETS = {
    "none": [],
    "minimal": [
        normalize_columns,
        strip_whitespace,
    ],
    "standard": [
        normalize_columns,
        strip_whitespace,
        drop_empty_rows,
        standardize_na,
    ],
    "aggressive": [
        normalize_columns,
        strip_whitespace,
        drop_empty_rows,
        drop_empty_cols,
        standardize_na,
        deduplicate,
        infer_types,
    ],
}
```

### API

```python
# Preset-based
fd = FileDrop("Data", clean="standard")

# Method-based
fd.clean("Data", normalize_columns=True, drop_empty_rows=True)

# Custom cleaner function
def my_cleaner(df, filename):
    # Custom logic
    return df

fd = FileDrop("Data", cleaner=my_cleaner)

# Cleaner chain
fd = FileDrop("Data", cleaners=[clean1, clean2, clean3])
```

---

## Phase 3: Combining DataFrames

### `combine()` Method

```python
def combine(
    self, 
    label: str,
    *,
    combiner: Optional[Callable] = None,
    add_source: bool = False,
    ignore_index: bool = True,
    include_metadata: Optional[List[str]] = None,
) -> pd.DataFrame:
    """
    Combine all DataFrames for a label into one.
    
    Args:
        label: Drop zone label
        combiner: Custom (data: Dict[str, DataFrame]) -> DataFrame
        add_source: Add '_source' column with filename
        ignore_index: Reset index after concat
        include_metadata: Metadata keys to add as columns
        
    Returns:
        Combined DataFrame
    """
```

### Default Behavior

```python
pd.concat(list(data.values()), ignore_index=True)
```

### With Source Tracking

```python
df = fd.combine("Data", add_source=True)
# Adds '_source' column with original filename
```

### With Metadata as Columns

```python
df = fd.combine("Data", include_metadata=["date", "author"])
# Adds '_date' and '_author' columns from extracted metadata
```

### Schema Mismatch Handling

| Scenario | Behavior |
|----------|----------|
| Identical columns | Direct concat |
| Subset columns | Concat with NaN fill, warn |
| Disjoint columns | Outer join or raise (configurable) |
| Type mismatches | Attempt coercion, warn |

---

## Integration with FileDrop

### Full Pipeline Example

```python
from ipyfiledrop import FileDrop

# Create with full pipeline enabled
fd = FileDrop(
    "Lab Data",
    retain_data=True,        # Accumulate files
    extract_core=True,       # Extract core tables
    clean="standard",        # Apply standard cleaning
).display()

# Drop multiple messy files or a ZIP archive
# Each file is automatically:
# 1. Parsed (CSV/Excel/etc)
# 2. Core extracted (density analysis)
# 3. Cleaned (normalize columns, etc)

# Access individual extracted data
result = fd.extract("Lab Data")
result.core       # DataFrame
result.metadata   # {"Report Date": "2024-01-15", ...}

# Combine all into single DataFrame
df = fd.combine("Lab Data", add_source=True, include_metadata=["date"])
```

### API Summary

| Method/Parameter | Description |
|------------------|-------------|
| `extract_core=True` | Enable automatic core extraction |
| `clean="standard"` | Apply cleaning preset |
| `cleaner=fn` | Custom cleaner function |
| `fd.extract(label)` | Get ExtractedData object |
| `fd.combine(label)` | Combine all DataFrames |
| `fd.clean(label)` | Apply cleaning in-place |

---

## Implementation Phases

### Phase 1: Core Extraction (Priority: High) - COMPLETED
- [x] Density analysis functions
- [x] Header detection
- [x] `extract_core_data()` function
- [x] `ExtractedData` dataclass
- [x] Basic metadata extraction
- [x] Integration with `IFrameDropWidget`

### Phase 2: Cleaning (Priority: High) - COMPLETED
- [x] Built-in cleaner functions
- [x] `clean` parameter presets
- [x] `cleaner=` custom function support
- [x] Cleaner factory functions (`make_normalize_columns`, `make_strip_whitespace`)
- [x] Cleaner options (`preserve_case`, `preserve_dashes`, `preserve_dots`, `normalize_inner`)

### Phase 3: Combining (Priority: Medium) - COMPLETED
- [x] `fd.combine()` method
- [x] `add_source` parameter
- [ ] `include_metadata` parameter (planned)
- [x] Schema mismatch handling (concat with NaN fill)

### Phase 4: Advanced (Priority: Low)
- [x] Confidence scoring
- [ ] Multi-table detection
- [ ] Transposed table detection
- [ ] Schema validation

---

## Test Data

Test files in `examples/data/sparse_messy/`:

| File | Description | Challenges |
|------|-------------|------------|
| `sample_log.csv` | Lab sample log | Header at row 5, IDs in col 2, footer rows |
| `sample_metadata.csv` | Sample metadata | IDs as column headers, transposed layout |

Additional test scenarios in `examples/data/`:
- `mixed_formats/` - Different ID formats
- `one_to_many/` - Relational data
- `sparse_overlap/` - Partial ID overlap
- `uid_detection/` - Various UID patterns

---

## References

- pandas concat: https://pandas.pydata.org/docs/reference/api/pandas.concat.html
- Similar tools: csvkit, pandas-profiling, dataprep
